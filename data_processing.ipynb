{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Takes raw data in csvs.\n",
    "Filters and creates datasets for specific time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from gensim.models import Phrases\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "\n",
    "def preprocess(filename=None, start_date=None, end_date=None):\n",
    "    # get data from csv\n",
    "    df = pd.read_csv(filename, index_col=\"id\", usecols=[\"id\", \"body\", \"author\", \"parent_id\", \"retrieved_on\", \"timestamp\"], parse_dates=['timestamp'])\n",
    "    # filter by date\n",
    "    if start_date:\n",
    "        df = df[df.timestamp >= start_date]\n",
    "    if end_date:\n",
    "        df = df[df.timestamp < end_date]\n",
    "\n",
    "    # remove rows where comment was deleted\n",
    "    df = df.drop(df[df.body == \"[deleted]\"].index)\n",
    "    df = df.drop(df[df.body == \"[removed]\"].index)\n",
    "\n",
    "    # remove links, convert to lowercase, tokenize, and remove short/long tokens\n",
    "    def remove_links_and_simple_preprocess(sentence):\n",
    "        sentence = re.sub(r'https?:\\/\\/.*', '', sentence)\n",
    "        return simple_preprocess(sentence)\n",
    "    simple_preprocessed = df['body'].astype(str).apply(remove_links_and_simple_preprocess)\n",
    "\n",
    "    # function to lemmatize each token, based on its part of speech\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def lemmatize_token(t: str, pos:str):\n",
    "        morphy_tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\n",
    "                  'VB':wn.VERB, 'RB':wn.ADV}\n",
    "        try:\n",
    "            pos = morphy_tag[pos[:2]]\n",
    "        except:\n",
    "            pos = None\n",
    "        if pos is not None:\n",
    "            lemma = lemmatizer.lemmatize(t, pos)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(t)\n",
    "        return lemma\n",
    "    \n",
    "    # tokenize comments, preserving common bigram phrases\n",
    "    # identify common bigram phrases\n",
    "    phrases = Phrases(simple_preprocessed, scoring=\"npmi\", threshold=0.7)\n",
    "    def preproccess_sentence(preprocessed_sentence: str) -> List[str]:\n",
    "        # combine tokens that make up a phrase and drop associated score\n",
    "        simple_tokens = [t[0] for t in phrases.analyze_sentence(preprocessed_sentence)]\n",
    "        # lemmatize tokens\n",
    "        tokens_and_pos = pos_tag(simple_tokens)\n",
    "        tokens = [lemmatize_token(t, pos) for t, pos in tokens_and_pos]\n",
    "        return tokens\n",
    "    \n",
    "    # create body_clean column: a preprocessed version of body\n",
    "    df['body_clean'] = simple_preprocessed.apply(preproccess_sentence)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"data/raw/\"\n",
    "datasets = {\n",
    "    \"incels\": \n",
    "        {\n",
    "            \"filename\": prefix + \"Incels_comments.csv\",\n",
    "            \"start_date\": \"2015-11-07\"\n",
    "        },\n",
    "    \"braincels\": \n",
    "        {\n",
    "            \"filename\": prefix + \"Braincels_comments.csv\",\n",
    "            \"start_date\": \"2017-09-30\"\n",
    "        },\n",
    "    \"trufemcels\": \n",
    "        {\n",
    "            \"filename\": prefix + \"Trufemcels_comments.csv\",\n",
    "            \"start_date\": \"2019-01-30\"\n",
    "        },\n",
    "    \"mensrights\": \n",
    "        {\n",
    "            \"filename\": prefix + \"MensRights_comments.csv\",\n",
    "            \"start_date\": \"2021-01-01\"\n",
    "        },\n",
    "    # add TheRedPill here\n",
    "    \"feminism_full\": \n",
    "        {\n",
    "            \"filename\": prefix + \"Feminism_comments.csv\", \n",
    "            \"start_date\": \"2015-11-07\"\n",
    "        },\n",
    "    \"fourthwavewomen\": \n",
    "        {\n",
    "            \"filename\": prefix + \"fourthwavewomen_comments.csv\",\n",
    "            \"start_date\": \"2021-01-01\"\n",
    "        }\n",
    "    # add IntersectionalFems here\n",
    "    # add Intersectionality here\n",
    "    # add RadicalFeminism here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/dn5ftycx50sgjnrrwnpdllth0000gn/T/ipykernel_2107/554965903.py:14: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename, index_col=\"id\", usecols=[\"id\", \"body\", \"author\", \"parent_id\", \"retrieved_on\", \"timestamp\"], parse_dates=['timestamp'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.762253761291504\n"
     ]
    }
   ],
   "source": [
    "for subreddit in datasets:\n",
    "    df = preprocess(**datasets[subreddit])\n",
    "    df.to_pickle(\"data/clean/\"+subreddit+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break feminism into time periods\n",
    "feminism_full = pd.read_pickle(\"data/clean/feminism_full.pkl\")\n",
    "feminism_chunks = []\n",
    "for i in range(2015, 2023, 2):\n",
    "    feminism_chunk = feminism_full[(feminism_full.timestamp >= str(i)+\"-01-01\") & (feminism_full.timestamp < str(i+2)+\"-01-01\")]\n",
    "    feminism_chunk.to_pickle(\"data/clean/feminism_\"+str(i)+\"_\"+str(i+2)+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine incels into one df\n",
    "incel_subreddits = [\"incels\", \"braincels\", \"trufemcels\", \"mensrights\"] # add TheRedPill here\n",
    "incel_dfs = [pd.read_pickle(\"data/clean/\"+subreddit+\".pkl\") for subreddit in incel_subreddits]\n",
    "full_df = pd.concat(incel_dfs)\n",
    "full_df.to_pickle(\"data/clean/incels_full.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "d38cpyp                        [reply, to, you, on, trucels]\n",
      "d38dsz9    [seduction, be, disastrous, for, woman, since,...\n",
      "d38e0dh    [thank, you, it, hearten, to, see, more, and, ...\n",
      "d3ej70j    [you, describe, yourself, a, non, chad, would,...\n",
      "d3ek59w    [rate, my, attractiveness, maybe, out, of, my,...\n",
      "Name: body_clean, dtype: object\n",
      "id\n",
      "doo7s2k    [why, do, not, have, fifty, subscriber, alread...\n",
      "doo85t6    [here, some, previous, essay, story, and, poet...\n",
      "dopbxoz                     [subscribe, do, get, an, upvote]\n",
      "dopbyxv    [hate, psychology, it, brainwash, gimmick, to,...\n",
      "dopcfvs    [damn, literally, study, cognitive, behavioral...\n",
      "Name: body_clean, dtype: object\n",
      "id\n",
      "efb6vdz    [isn, the, girl, from, vampire, diary, bulgari...\n",
      "efb6zku    [even, for, satire, eww, blehh, doesn, the, av...\n",
      "efb72bt    [eh, wouldn, say, beautiful, woman, feel, unco...\n",
      "efb7b2g    [yeah, not, say, she, doesn, face, her, own, i...\n",
      "efb7c7o                                  [all, the, fresher]\n",
      "Name: body_clean, dtype: object\n",
      "id\n",
      "ghnn77t    [glad, to, see, some, country, be, reform, it,...\n",
      "ghnn8n6    [not, sure, this, woman, be, the, best, spokes...\n",
      "ghnn9jp    [it, common, for, woman, to, rely, upon, plaus...\n",
      "ghnnldv    [you, haven, see, any, of, her, video, other, ...\n",
      "ghno0jg                                               [good]\n",
      "Name: body_clean, dtype: object\n",
      "id\n",
      "d38cpyp                        [reply, to, you, on, trucels]\n",
      "d38dsz9    [seduction, be, disastrous, for, woman, since,...\n",
      "d38e0dh    [thank, you, it, hearten, to, see, more, and, ...\n",
      "d3ej70j    [you, describe, yourself, a, non, chad, would,...\n",
      "d3ek59w    [rate, my, attractiveness, maybe, out, of, my,...\n",
      "Name: body_clean, dtype: object\n",
      "id\n",
      "cwrahh1    [there, an, article, by, wendy, faulkner, unfo...\n",
      "cwralnk    [lot, of, people, might, think, it, graphic, t...\n",
      "cwraz6x    [people, often, hate, on, feminist, for, take,...\n",
      "cwrb08a    [ve, read, before, that, these, farm, be, also...\n",
      "cwrb4vq                                          [this, one]\n",
      "Name: body_clean, dtype: object\n",
      "id\n",
      "cwrahh1    [there, an, article, by, wendy, faulkner, unfo...\n",
      "cwralnk    [lot, of, people, might, think, it, graphic, t...\n",
      "cwraz6x    [people, often, hate, on, feminist, for, take,...\n",
      "cwrb08a    [ve, read, before, that, these, farm, be, also...\n",
      "cwrb4vq                                          [this, one]\n",
      "Name: body_clean, dtype: object\n",
      "id\n",
      "dbunawq    [ok, go, to, base, my, understanding, of, thes...\n",
      "dbuniis    [both, woman, and, men, have, pubic, hair, so,...\n",
      "dbuo7i4    [write, by, someone, who, doesn, know, what, t...\n",
      "dbuo87u    [really, wish, more, people, would, understand...\n",
      "dbuo883    [it, because, society, have, say, so, for, ver...\n",
      "Name: body_clean, dtype: object\n",
      "id\n",
      "eczbeqn                                             [remove]\n",
      "eczbmjz                                             [remove]\n",
      "eczbtzw                                             [remove]\n",
      "eczc70x                                             [remove]\n",
      "eczc7y0    [whoa_whoa, whoa_whoa, hold, up, woman, like, ...\n",
      "Name: body_clean, dtype: object\n",
      "id\n",
      "ghnopkr                                     [pretty, common]\n",
      "ghnpmud    [how, do, that, work, why, don, they, worry, a...\n",
      "ghnq4ty                                             [remove]\n",
      "ghnqa7m                                             [remove]\n",
      "ghnqkqi                                             [remove]\n",
      "Name: body_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def load_df(subreddit, prefix=\"data/clean/\"):\n",
    "    df = pd.read_pickle(prefix+subreddit+\".pkl\")\n",
    "    return df\n",
    "subreddits = [\"incels\", \"braincels\", \"trufemcels\", \"mensrights\", \"incels_full\",\"feminism_full\", \"feminism_2015_2017\", \"feminism_2017_2019\", \"feminism_2019_2021\", \"feminism_2021_2023\"]\n",
    "for subreddit in subreddits:\n",
    "    print(load_df(subreddit)[\"body_clean\"].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
